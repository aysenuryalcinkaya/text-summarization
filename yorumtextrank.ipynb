{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    category                                               text\n",
      "0  alisveris  A101'den Ürünüm Gelmedi!,Sizden 14/05/2020 tar...\n",
      "1  alisveris  Teknosa Preo Mouse Driver Sorunu,Ben yaklaşık ...\n",
      "2  alisveris  Vatan Bilgisayar Ücret İadesi Sorunu,\"08 Mayıs...\n",
      "3  alisveris  Vatan Bilgisayar Hala Kargoya Verilmemesi,\"29 ...\n",
      "4  alisveris  Migros'ta  Kıyma Çok Kötü,Yarım kg kıyma aldık...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV dosyasını oku\n",
    "data = pd.read_csv(\"sampled_data.csv\")\n",
    "\n",
    "# Veriye genel bir bakış\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Özel karakterleri, sembolleri ve sayıları temizle\n",
    "    text = re.sub(r\"[^a-zA-ZüöçğışıĞÜÖÇİŞ ]\", \"\", text)\n",
    "    # Birden fazla boşluğu tek boşluğa indir\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Metinleri temizle\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Türkçe stop word listesi\n",
    "turkish_stopwords = set(stopwords.words(\"turkish\"))\n",
    "\n",
    "def process_text(text):\n",
    "    sentences = sent_tokenize(text)  # Metni cümlelere bölelim\n",
    "    \n",
    "    # Her cümleyi kelimelere bölelim ve stop words'leri çıkaralım\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if word.lower() not in turkish_stopwords]\n",
    "        processed_sentences.append(filtered_words)\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "# Cümleleri ve stop words'leri işle\n",
    "data['processed_text'] = data['cleaned_text'].apply(process_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acil  acilen  ade     adesi  adet  adevamını  adres  adı  adına  aile  ...  \\\n",
      "0   0.0     0.0  0.0  0.000000   0.0        0.0    0.0  0.0    0.0   0.0  ...   \n",
      "1   0.0     0.0  0.0  0.000000   0.0        0.0    0.0  0.0    0.0   0.0  ...   \n",
      "2   0.0     0.0  0.0  0.229632   0.0        0.0    0.0  0.0    0.0   0.0  ...   \n",
      "3   0.0     0.0  0.0  0.000000   0.0        0.0    0.0  0.0    0.0   0.0  ...   \n",
      "4   0.0     0.0  0.0  0.000000   0.0        0.0    0.0  0.0    0.0   0.0  ...   \n",
      "\n",
      "   şirket  şirketi  şok  şubat  şube  şubesi  şubesinde  şubesinden  şubesine  \\\n",
      "0     0.0      0.0  0.0    0.0   0.0     0.0        0.0         0.0       0.0   \n",
      "1     0.0      0.0  0.0    0.0   0.0     0.0        0.0         0.0       0.0   \n",
      "2     0.0      0.0  0.0    0.0   0.0     0.0        0.0         0.0       0.0   \n",
      "3     0.0      0.0  0.0    0.0   0.0     0.0        0.0         0.0       0.0   \n",
      "4     0.0      0.0  0.0    0.0   0.0     0.0        0.0         0.0       0.0   \n",
      "\n",
      "   şubeye  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vektörleştiriciyi oluştur\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # En fazla 1000 kelime kullanacağız\n",
    "\n",
    "# İşlenmiş cümleleri birleştirerek metinlere dönüştür\n",
    "corpus = [' '.join(words) for words in data['processed_text'].apply(lambda words: [' '.join(sentence) for sentence in words])]\n",
    "\n",
    "# TF-IDF matrisini oluştur\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# TF-IDF matrisini DataFrame'e çevir\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Sonuçları incele\n",
    "print(tfidf_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.03662926 0.04794504 0.08134647 0.04771625]\n",
      " [0.03662926 1.         0.02268624 0.02091655 0.00314593]\n",
      " [0.04794504 0.02268624 1.         0.19991972 0.        ]\n",
      " [0.08134647 0.02091655 0.19991972 1.         0.        ]\n",
      " [0.04771625 0.00314593 0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cümle benzerlik matrisini oluştur\n",
    "sentence_similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Sonuçları incele (örneğin ilk 5 cümle arası benzerlik)\n",
    "print(sentence_similarity_matrix[:5, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.02167297 0.02019878 0.02305615 0.0020506 ]\n",
      " [0.02167297 1.         0.02119619 0.00905196 0.01524126]\n",
      " [0.02019878 0.02119619 1.         0.01932495 0.03948261]\n",
      " [0.02305615 0.00905196 0.01932495 1.         0.02141202]\n",
      " [0.0020506  0.01524126 0.03948261 0.02141202 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Kelime benzerlik matrisini oluştur (sütunları transpoz yaparak kullanıyoruz)\n",
    "word_similarity_matrix = cosine_similarity(tfidf_matrix.T)\n",
    "\n",
    "# Sonuçları incele (örneğin ilk 5 kelime arası benzerlik)\n",
    "print(word_similarity_matrix[:5, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Cümle benzerlik matrisini kullanarak bir graf oluştur\n",
    "def create_sentence_graph(similarity_matrix):\n",
    "    num_sentences = similarity_matrix.shape[0]\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Düğümleri ekle\n",
    "    graph.add_nodes_from(range(num_sentences))\n",
    "\n",
    "    # Kenarları ekle\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            graph.add_edge(i, j, weight=similarity)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Kelime benzerlik matrisini kullanarak bir graf oluştur\n",
    "def create_word_graph(similarity_matrix, words):\n",
    "    num_words = similarity_matrix.shape[0]\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Düğümleri ekle\n",
    "    graph.add_nodes_from(range(num_words))\n",
    "\n",
    "    # Kelimeleri düğümlere eşle\n",
    "    node_labels = {i: word for i, word in enumerate(words)}\n",
    "    graph = nx.relabel_nodes(graph, node_labels)\n",
    "\n",
    "    # Kenarları ekle\n",
    "    for i in range(num_words):\n",
    "        for j in range(i + 1, num_words):\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            graph.add_edge(words[i], words[j], weight=similarity)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Örnek graf oluşturma (cümle benzerlik matrisi kullanarak)\n",
    "sentence_graph = create_sentence_graph(sentence_similarity_matrix)\n",
    "# Örnek graf oluşturma (kelime benzerlik matrisi kullanarak)\n",
    "word_graph = create_word_graph(word_similarity_matrix, tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Grafın düğüm sayısı ve kenar sayısı\n",
    "print(\"Sentence Graph Nodes:\", sentence_graph.number_of_nodes())\n",
    "print(\"Sentence Graph Edges:\", sentence_graph.number_of_edges())\n",
    "print(\"Word Graph Nodes:\", word_graph.number_of_nodes())\n",
    "print(\"Word Graph Edges:\", word_graph.number_of_edges())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
